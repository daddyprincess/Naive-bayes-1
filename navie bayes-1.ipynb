{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f45053-325b-45d7-a606-a87d97bbf93e",
   "metadata": {},
   "source": [
    "## Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c811377-d206-45e5-a21c-9440be5fd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem, named after the 18th-century statistician and philosopher Thomas Bayes, is a fundamental theorem in \n",
    "probability theory and statistics. It describes how to update or revise our beliefs about a hypothesis or an event in \n",
    "light of new evidence. In essence, it quantifies how our degree of belief in a hypothesis should change when we obtain new \n",
    "information.\n",
    "\n",
    "Bayes' theorem is commonly expressed in the following mathematical form:\n",
    "\n",
    "            P(A∣B)=  P(B∣A)⋅P(A) / p(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "    ~P(A∣B) is the posterior probability of event or hypothesis A given evidence B.\n",
    "    ~P(B∣A) is the likelihood or probability of observing evidence B given that event A is true.\n",
    "    ~P(A) is the prior probability of event A, which represents our initial belief in A before considering evidence B.\n",
    "    ~P(B) is the marginal probability of evidence B, which is the total probability of observing evidence B, regardless of\n",
    "     whether A is true or not.\n",
    "        \n",
    "In plain terms, Bayes' theorem allows us to update our beliefs about the probability of an event or hypothesis A (the \n",
    "posterior probability) based on new evidence B. It does so by taking into account our prior beliefs (prior probability) and\n",
    "how likely we would observe the evidence under those prior beliefs (likelihood). This theorem has wide-ranging applications\n",
    "in various fields, including statistics, machine learning, Bayesian inference, and artificial intelligence, where it is used\n",
    "for tasks such as Bayesian modeling, spam filtering, medical diagnosis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12beb0-600a-4370-9ca0-dbb5f622f214",
   "metadata": {},
   "source": [
    "## Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d41e4-9d4a-4fc3-84db-2a05a569d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The formula for Bayes' theorem is as form:\n",
    "    \n",
    "        P(A∣B)= P(B∣A)⋅P(A) / P(B)\n",
    "Where:\n",
    "\n",
    "    ~P(A∣B) is the posterior probability of event or hypothesis A given evidence B.\n",
    "    ~P(B∣A) is the likelihood or probability of observing evidence B given that event A is true.\n",
    "    ~P(A) is the prior probability of event A, which represents our initial belief in A before considering evidence B.\n",
    "    ~P(B) is the marginal probability of evidence B, which is the total probability of observing evidence B, regardless of \n",
    "     whether A is true or not.\n",
    "        \n",
    "This formula quantifies how our degree of belief in a hypothesis or event A should change when we obtain new evidence B. It\n",
    "is a fundamental tool in Bayesian statistics and inference, allowing us to update our beliefs based on data and make\n",
    "probabilistic predictions and decisions in various fields of science, engineering, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbed2cd-3e92-4de6-8bf0-a5c9d05cbf35",
   "metadata": {},
   "source": [
    "## Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aaf237-fe45-4308-8b9e-f5ec9ecde1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is used in practice in a wide range of fields and applications to make probabilistic predictions, update\n",
    "beliefs, and perform statistical inference. Here are some common ways Bayes' theorem is used in practice:\n",
    "\n",
    "1.Statistical Inference: Bayes' theorem is fundamental to Bayesian statistics, where it is used to update prior beliefs with \n",
    "  new data, resulting in posterior probability distributions. This allows for more flexible and coherent statistical modeling\n",
    "and hypothesis testing.\n",
    "\n",
    "2.Machine Learning: In machine learning, Bayes' theorem is used in Bayesian machine learning methods, such as Bayesian \n",
    "  networks and Bayesian classifiers. These methods use probabilistic models to make predictions and handle uncertainty\n",
    "effectively.\n",
    "\n",
    "3.Medical Diagnosis: Bayes' theorem is used to calculate the probability of a medical condition or disease given specific\n",
    "  symptoms or test results. It's especially useful when dealing with complex medical diagnoses where multiple factors need\n",
    "to be considered.\n",
    "\n",
    "4.Spam Filtering: In email spam filters, Bayes' theorem is used to classify emails as spam or not spam based on the\n",
    "  probability of certain words or phrases appearing in the message. This approach is known as Bayesian spam filtering.\n",
    "\n",
    "5.Natural Language Processing: In natural language processing, Bayes' theorem can be used for various tasks, including text\n",
    "  classification, sentiment analysis, and language modeling.\n",
    "\n",
    "6.A/B Testing: When conducting A/B tests to compare two or more versions of a product or website, Bayes' theorem can be \n",
    "  used to calculate the probability that one version is better than another based on the observed data.\n",
    "\n",
    "7.Weather Forecasting: In meteorology, Bayes' theorem is used in data assimilation to combine measurements from various\n",
    "  sources (e.g., weather stations, satellites) with numerical weather models to make more accurate weather forecasts.\n",
    "\n",
    "8.Risk Assessment: Bayes' theorem is used in risk assessment and decision-making processes to calculate the probability of\n",
    "  various outcomes and inform decisions in areas such as finance, insurance, and project management.\n",
    "\n",
    "9.Image and Speech Recognition: In computer vision and speech recognition, Bayes' theorem can be used to model and recognize\n",
    "  patterns in images, audio, and speech data.\n",
    "\n",
    "10.Biostatistics: In biology and genetics, Bayes' theorem is applied to problems like genetic mapping, population genetics,\n",
    "  and phylogenetics.\n",
    "\n",
    "The key idea in all these applications is to use Bayes' theorem to update prior beliefs or probabilities with new evidence,\n",
    "resulting in more informed and probabilistically sound decisions. Bayes' theorem provides a powerful framework for dealing\n",
    "with uncertainty and making predictions based on a combination of prior knowledge and observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4cdfad-1557-4c62-ad76-f142abe255f5",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82305e80-8302-4df5-8aaa-7450d3961d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem and conditional probability are closely related concepts in probability theory and statistics. Bayes' \n",
    "theorem is essentially an extension of conditional probability that allows us to update our beliefs about an event or \n",
    "hypothesis based on new evidence.\n",
    "\n",
    "Conditional Probability:\n",
    "    \n",
    "    ~Conditional probability represents the probability of an event occurring given that another event has already \n",
    "     occurred. It is denoted as P(A|B), where A is the event of interest, and B is the condition or evidence. The formula \n",
    "    for conditional probability is:\n",
    "\n",
    "        P(A∣B)= P(A∩B) / P(B)\n",
    "\n",
    "This formula calculates the probability of event A happening under the condition or evidence of event B.\n",
    "\n",
    "Bayes' Theorem:\n",
    "\n",
    "    ~Bayes' theorem, on the other hand, is a more general framework that allows us to update our prior beliefs (prior \n",
    "     probability) about an event or hypothesis in light of new evidence. It uses conditional probability to do this. The\n",
    "    formula for Bayes' theorem is:\n",
    "\n",
    "        P(A∣B)= P(B∣A)⋅P(A) / P(B)\n",
    "\n",
    "In this formula:\n",
    "\n",
    "    ~P(A|B) is the posterior probability of event A given evidence B.\n",
    "    ~P(B|A) is the likelihood or probability of observing evidence B given that event A is true.\n",
    "    ~P(A) is the prior probability of event A, representing our initial belief.\n",
    "    ~P(B) is the marginal probability of evidence B, the total probability of observing B.\n",
    "    \n",
    "The relationship between Bayes' theorem and conditional probability is evident in the fact that Bayes' theorem uses\n",
    "conditional probability (P(B|A)) to calculate the posterior probability (P(A|B)). It allows us to update our beliefs by \n",
    "relating the probability of observing evidence (B) given our hypothesis (A) to the probability of our hypothesis being true\n",
    "given the evidence.\n",
    "\n",
    "In summary, Bayes' theorem is a powerful tool for updating probabilities based on new information or evidence, and it relies\n",
    "on conditional probability to express how beliefs should change in light of that evidence. Conditional probability is a\n",
    "fundamental component of Bayes' theorem, providing the basis for conditional inference and probabilistic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99f460-e5f2-416a-ad54-f55ed81cd904",
   "metadata": {},
   "source": [
    "## Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c5380-23d2-416d-924f-8ac1a20d71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on the nature of the data and the \n",
    "assumptions you are willing to make about the independence of features. There are three common types of Naive Bayes\n",
    "classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here are some guidelines to help\n",
    "you make the right choice:\n",
    "\n",
    "1.Gaussian Naive Bayes (GNB):\n",
    "\n",
    "    ~Continuous Data: Use GNB when your features are continuous (real-valued) and can be reasonably assumed to follow a\n",
    "     Gaussian (normal) distribution.\n",
    "    ~Independence Assumption: GNB assumes that the features within each class are normally distributed and that they have\n",
    "     different means and variances. If this assumption is not too far from reality, GNB can work well.\n",
    "        \n",
    "2.Multinomial Naive Bayes (MNB):\n",
    "\n",
    "    ~Discrete Data: MNB is suitable for text data or any other type of data where features are discrete and represent\n",
    "     counts or frequencies (e.g., word counts in text classification).\n",
    "    ~Text Classification: It is commonly used in text classification problems like spam detection, sentiment analysis, \n",
    "     and topic classification.\n",
    "        \n",
    "3.Bernoulli Naive Bayes (BNB):\n",
    "\n",
    "    ~Binary Data: Use BNB when your features are binary, meaning they take on only two values (e.g., 0 or 1, true or false).\n",
    "    ~Feature Presence/Absence: BNB is often used for problems where you're interested in whether a particular feature is \n",
    "     present or absent in a data point (e.g., document classification based on the presence of certain keywords).\n",
    "        \n",
    "Here are additional considerations to help you choose:\n",
    "\n",
    "Data Distribution: Consider the distribution of your data. If your data doesn't strictly follow the assumptions of any Naive \n",
    "Bayes variant, you might still try different variants and see which one works better empirically.\n",
    "\n",
    "Feature Independence Assumption: Keep in mind that all Naive Bayes classifiers make a strong assumption of feature\n",
    "independence within each class. This assumption may or may not hold in your specific problem, so it's essential to assess it.\n",
    "\n",
    "Size of Data: The size of your dataset can also influence the choice. If you have a small dataset, simpler models like Naive\n",
    "Bayes can be more robust and less prone to overfitting.\n",
    "\n",
    "Prior Knowledge: Your prior knowledge about the data and the problem can guide your choice. For example, if you have domain-\n",
    "specific knowledge that certain features are more likely to be independent, that can inform your decision.\n",
    "\n",
    "Experimentation: In practice, it's often a good idea to try different Naive Bayes variants and compare their performance\n",
    "using cross-validation or other evaluation techniques to see which one works best for your specific problem.\n",
    "\n",
    "Ultimately, the choice of Naive Bayes classifier depends on the characteristics of your data and the specific assumptions\n",
    "you are willing to make about the independence of features. It's often a matter of experimentation and assessing the results\n",
    "in the context of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82cd81-6868-49c5-bc1e-3873e72a7461",
   "metadata": {},
   "source": [
    "## Q6. Assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330679b3-25ed-4763-be40-d9f950accb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "To predict the class of a new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we can calculate the posterior\n",
    "probabilities for each class (A and B) and then choose the class with the higher probability. Since we are assuming equal\n",
    "prior probabilities for each class (P(A) = P(B)), we can ignore the priors in this case.\n",
    "\n",
    "To calculate the posterior probabilities, we'll use the Naive Bayes formula:\n",
    "\n",
    "            P(Class∣X1=3,X2=4)∝P(X1=3∣Class)⋅P(X2=4∣Class)\n",
    "\n",
    "We'll calculate this probability for both classes A and B and choose the class with the higher value.\n",
    "\n",
    "For Class A:\n",
    "\n",
    "            P(X1=3∣A) = 4/13 (Based on the table)\n",
    "            P(X2=4∣A) = 3/13 (Based on the table)\n",
    "For Class B:\n",
    "\n",
    "            P(X1=3∣B) = 1/7 (Based on the table)\n",
    "            P(X2=4∣B) = 3/7 (Based on the table)\n",
    "Now, let's calculate the posterior probabilities:\n",
    "\n",
    "For Class A:\n",
    "\n",
    "            P(Class=A∣X1=3,X2=4)∝ 4/13.3/13 = 12/169\n",
    "\n",
    "For Class B:\n",
    "\n",
    "            P(Class=B∣X1=3,X2=4)∝ 1/7.3/7 = 3/49\n",
    "\n",
    "Comparing the posterior probabilities:\n",
    "\n",
    "            P(Class=A∣X1=3,X2=4) = 12/169\n",
    "            P(Class=B∣X1=3,X2=4) = 3/49\n",
    "\n",
    "Since P(Class=A∣X1=3,X2=4) is greater than P(Class=B∣X1=3,X2=4), Naive Bayes would predict that the new instance belongs to\n",
    "Class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
